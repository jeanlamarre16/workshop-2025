Agis comme un architecte DevOps senior chargé de concevoir, documenter et livrer
une infrastructure big data entièrement dockerisée, prête à déployer en
environnement de développement (Docker Compose) et de production (Docker Swarm),
avec monitoring, pare-feu, haute disponibilité, sauvegardes et PRA complet.

Ta réponse doit être auto-suffisante (tout dans un seul message), reproductible
et immédiatement exploitable.

OBJECTIF GÉNÉRAL

Construire une plateforme big data incluant :
- Un outil de ticketing (GLPI)
- Un outil d'historisation de données (OpenSearch)
- Un système de monitoring complet (Prometheus/Grafana)
- Un datalake (Cassandra)
- Un pare-feu applicatif (Traefik + CrowdSec)
- Des mesures de haute disponibilité
- Un plan de sauvegarde et de reprise d'activité (PRA) complet
- Une documentation exhaustive de l'architecture et des configurations

IMAGES DOCKER FIXÉES (VERSIONS PINNÉES - PAS DE :latest)

REVERSE PROXY & SÉCURITÉ :
- traefik:2.10.4
- crowdsecurity/crowdsec:v1.6.2
- fbonalair/traefik-crowdsec-bouncer:0.4.0

TICKETING :
- glpi/glpi:11.0.1
- mariadb:10.11.3

HISTORISATION & SEARCH :
- opensearchproject/opensearch:2.15.0
- opensearchproject/opensearch-dashboards:2.15.0

DATALAKE :
- cassandra:4.1.9

MONITORING :
- prom/prometheus:v2.48.0
- prom/alertmanager:v0.25.0
- grafana/grafana:9.5.2
- prom/node-exporter:v1.6.1
- gcr.io/cadvisor/cadvisor:v0.47.2
- prom/blackbox-exporter:v0.24.0
- quay.io/prometheuscommunity/elasticsearch-exporter:v1.7.0
- criteord/cassandra_exporter:2.3.8 (ou alternative disponible)

LOGGING :
- grafana/loki:2.8.1
- grafana/promtail:2.8.1

STOCKAGE & BACKUPS :
- minio/minio:RELEASE.2023-11-20T22-40-07Z
- restic/restic:0.17.0

INFRASTRUCTURE :
- consul:1.14 (pour Loki ring en dev)

CONFIGURATIONS DE VOLUMES (PRÉCISES)

VOLUMES NOMMÉS DOCKER (docker-compose et stack) :

Base de données & Applications :
  db_data:
    - Point de montage : /var/lib/mysql (MariaDB)
    - Usage : Base de données GLPI
    - Backup : Obligatoire via mysqldump + restic
    - Driver : local

  glpi_data:
    - Point de montage : /var/www/html (GLPI)
    - Usage : Fichiers et plugins GLPI
    - Backup : Obligatoire
    - Driver : local

OpenSearch Cluster (3 nœuds) :
  opensearch1_data:
    - Point de montage : /usr/share/opensearch/data
    - Usage : Données et index OpenSearch nœud 1
    - Backup : Snapshot API vers MinIO
    - Driver : local

  opensearch2_data:
    - Point de montage : /usr/share/opensearch/data
    - Usage : Données et index OpenSearch nœud 2
    - Backup : Snapshot API vers MinIO
    - Driver : local

  opensearch3_data:
    - Point de montage : /usr/share/opensearch/data
    - Usage : Données et index OpenSearch nœud 3
    - Backup : Snapshot API vers MinIO
    - Driver : local

Cassandra Cluster (3 nœuds) :
  cassandra1_data:
    - Point de montage : /var/lib/cassandra
    - Usage : Données Cassandra nœud 1 (SSTables, commitlog)
    - Backup : Snapshot + nodetool vers MinIO
    - Driver : local

  cassandra2_data:
    - Point de montage : /var/lib/cassandra
    - Usage : Données Cassandra nœud 2
    - Backup : Snapshot + nodetool vers MinIO
    - Driver : local

  cassandra3_data:
    - Point de montage : /var/lib/cassandra
    - Usage : Données Cassandra nœud 3
    - Backup : Snapshot + nodetool vers MinIO
    - Driver : local

Monitoring & Métriques :
  prometheus_data:
    - Point de montage : /prometheus
    - Usage : Données TSDB Prometheus
    - Rétention : 30 jours
    - Backup : Optionnel (données volatiles)
    - Driver : local

  grafana_data:
    - Point de montage : /var/lib/grafana
    - Usage : Dashboards, datasources, plugins Grafana
    - Backup : Obligatoire (dashboards custom)
    - Driver : local

Logging :
  loki_data:
    - Point de montage : /loki
    - Usage : Chunks, WAL, index Loki
    - Rétention : 30 jours (720h)
    - Backup : Optionnel
    - Driver : local

Stockage Objet :
  minio_data:
    - Point de montage : /data
    - Usage : Buckets MinIO (backups restic)
    - Backup : Réplication externe obligatoire
    - Driver : local

BIND MOUNTS (fichiers de configuration - READ-ONLY quand possible) :

Traefik :
  - ./traefik/acme:/acme (RW - certificats ACME)
  - ./traefik/dynamic:/dynamic:ro (configurations dynamiques)
  - /var/run/docker.sock:/var/run/docker.sock:ro (discovery)

CrowdSec :
  - ./crowdsec/data:/var/lib/crowdsec/data (base de données locale)
  - ./crowdsec/config:/etc/crowdsec (configurations)
  - /var/log:/var/log:ro (logs système)

Prometheus :
  - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
  - ./prometheus/rules:/etc/prometheus/rules:ro (alert rules)

Alertmanager :
  - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro

Grafana :
  - ./grafana/provisioning:/etc/grafana/provisioning:ro
    Contient :
      - datasources/prometheus.yml
      - datasources/loki.yml
      - dashboards/dashboards.yml
      - dashboards/*.json

Loki :
  - ./loki/loki-config.yml:/etc/loki/local-config.yaml:ro

Promtail :
  - ./promtail/promtail-config.yml:/etc/promtail/config.yml:ro
  - /var/lib/docker/containers:/var/lib/docker/containers:ro
  - /var/log:/var/log:ro

Node Exporter :
  - /proc:/host/proc:ro
  - /sys:/host/sys:ro
  - /:/rootfs:ro

cAdvisor :
  - /:/rootfs:ro
  - /var/run:/var/run:rw
  - /sys:/sys:ro
  - /var/lib/docker/:/var/lib/docker:ro

Cassandra (si configs custom) :
  - ./cassandra/cassandra1.yaml:/etc/cassandra/cassandra.yaml:ro
  - ./cassandra/cassandra2.yaml:/etc/cassandra/cassandra.yaml:ro
  - ./cassandra/cassandra3.yaml:/etc/cassandra/cassandra.yaml:ro

OpenSearch (si configs custom) :
  - ./opensearch/opensearch.yml:/usr/share/opensearch/config/opensearch.yml:ro

ARCHITECTURE RÉSEAU DOCKER

RÉSEAUX OVERLAY (Docker Swarm) / BRIDGE (Docker Compose) :

front:
  - Driver : overlay (prod) / bridge (dev)
  - Usage : Services exposés publiquement via Traefik
  - Services : traefik, glpi, grafana, opensearch-dashboards, crowdsec, traefik-bouncer
  - Encryption : Oui (--opt encrypted)

back:
  - Driver : overlay (prod) / bridge (dev)
  - Usage : Services internes de monitoring
  - Services : prometheus, alertmanager, grafana, loki, promtail, blackbox, cadvisor
  - Encryption : Oui

data:
  - Driver : overlay (prod) / bridge (dev)
  - Usage : Backends de données
  - Services : mariadb, opensearch1, opensearch2, opensearch3, cassandra1, cassandra2, cassandra3, minio, opensearch-exporter
  - Encryption : Oui

consul_net:
  - Driver : overlay (prod) / bridge (dev)
  - Usage : Service discovery Loki (dev seulement)
  - Services : consul, loki

ISOLATION :
- Traefik ne peut communiquer qu'avec front
- Prometheus scrape via back et data
- Databases isolées sur data
- Pas de communication directe front <-> data (via back uniquement)

SECRETS DOCKER (OBLIGATOIRES)

Secrets à créer dans ./secrets/ (dev) ou via docker secret create (prod) :

1. mariadb_root_password.txt
   - Usage : Root password MariaDB
   - Génération : openssl rand -base64 32

2. glpi_db_password.txt
   - Usage : GLPI database password
   - Génération : openssl rand -base64 32

3. grafana_admin_password.txt
   - Usage : Grafana admin password
   - Génération : openssl rand -base64 32

4. minio_root_password.txt
   - Usage : MinIO root password
   - Génération : openssl rand -base64 32

5. restic_password.txt
   - Usage : Chiffrement backups restic
   - Génération : openssl rand -base64 32

6. opensearch_admin_password.txt (si nécessaire)
   - Usage : OpenSearch admin password
   - Génération : openssl rand -base64 32

MONTAGE DES SECRETS :
- Point de montage : /run/secrets/<secret_name>
- Permissions : 0400
- Variables d'environnement : *_FILE pour pointer vers le secret

CONFIGURATIONS DÉTAILLÉES PAR SERVICE

TRAEFIK :
  Ports : 80, 443, 8080 (dashboard dev only)
  Command :
    - --providers.docker=true
    - --providers.docker.exposedbydefault=false
    - --providers.docker.swarmMode=true (prod only)
    - --providers.file.directory=/dynamic
    - --entrypoints.web.address=:80
    - --entrypoints.websecure.address=:443
    - --entrypoints.web.http.redirections.entryPoint.to=websecure
    - --entrypoints.web.http.redirections.entryPoint.scheme=https
    - --certificatesresolvers.le.acme.email=${TRAEFIK_ACME_EMAIL}
    - --certificatesresolvers.le.acme.storage=/acme/acme.json
    - --certificatesresolvers.le.acme.tlschallenge=true
    - --api=true
    - --api.dashboard=true
    - --metrics.prometheus=true
    - --log.level=INFO
    - --accesslog=true
  Middlewares (dans /dynamic/middlewares.yml) :
    - securityHeaders : HSTS, X-Frame-Options, CSP
    - ratelimit : 100 req/s burst 50
    - crowdsec-forwardauth : intégration bouncer
  Deploy :
    replicas : 2 (prod)
    placement : spread managers
    resources :
      limits : cpu 0.5, memory 512M
      reservations : cpu 0.25, memory 256M

CROWDSEC :
  Volumes :
    - ./crowdsec/data:/var/lib/crowdsec/data
    - ./crowdsec/config:/etc/crowdsec
    - /var/log:/var/log:ro
  Environment :
    - COLLECTIONS=crowdsecurity/traefik crowdsecurity/http-cve
    - PARSERS=crowdsecurity/traefik-logs
  Cap_drop : ALL
  Cap_add : [] (aucune)
  Healthcheck : cscli lapi status

TRAEFIK-BOUNCER :
  Environment :
    - CROWDSEC_BOUNCER_API_KEY (généré via cscli bouncers add)
    - CROWDSEC_AGENT_HOST=crowdsec:8080
    - GIN_MODE=release (prod)
  Depends_on : crowdsec
  Healthcheck : curl -f http://localhost:8080/api/v1/healthz

MARIADB :
  Image : mariadb:10.11.3
  Command : --transaction-isolation=READ-COMMITTED --binlog-format=ROW --innodb-buffer-pool-size=256M
  Environment :
    - MYSQL_ROOT_PASSWORD_FILE=/run/secrets/mariadb_root_password
    - MYSQL_DATABASE=${GLPI_DB_NAME}
    - MYSQL_USER=${GLPI_DB_USER}
    - MYSQL_PASSWORD_FILE=/run/secrets/glpi_db_password
  Volumes :
    - db_data:/var/lib/mysql
    - ./mariadb/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
  Healthcheck :
    test : mysqladmin ping -u root --password=$(cat /run/secrets/mariadb_root_password)
    interval : 30s
    timeout : 10s
    retries : 5
  Deploy :
    replicas : 1
    placement : constraints [node.role == manager]
    resources :
      limits : cpu 1.0, memory 1G

GLPI :
  Image : glpi/glpi:11.0.1
  Environment :
    - GLPI_DB_HOST=mariadb
    - GLPI_DB_USER=${GLPI_DB_USER}
    - GLPI_DB_PASSWORD_FILE=/run/secrets/glpi_db_password
    - GLPI_DB_NAME=${GLPI_DB_NAME}
  Volumes :
    - glpi_data:/var/www/html
  Labels Traefik :
    - traefik.enable=true
    - traefik.http.routers.glpi.rule=Host(`glpi.${DOMAIN}`)
    - traefik.http.routers.glpi.entrypoints=websecure
    - traefik.http.routers.glpi.tls.certresolver=le
    - traefik.http.routers.glpi.middlewares=securityHeaders@file,ratelimit@file,crowdsec-forwardauth@docker
    - traefik.http.services.glpi.loadbalancer.server.port=80
  Depends_on : mariadb
  Healthcheck :
    test : curl -f http://localhost || exit 1
    interval : 30s
  Deploy :
    replicas : 2 (prod)
    update_config : parallelism 1, delay 30s
    resources :
      limits : cpu 0.5, memory 1G

OPENSEARCH (cluster 3 nœuds) :
  Image : opensearchproject/opensearch:2.15.0
  Environment (commun) :
    - cluster.name=opensearch-cluster
    - discovery.seed_hosts=opensearch1,opensearch2,opensearch3
    - cluster.initial_master_nodes=opensearch1,opensearch2,opensearch3
    - bootstrap.memory_lock=true
    - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
    - OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_ADMIN_PASSWORD}
    - plugins.security.disabled=false
    - plugins.security.ssl.http.enabled=true
    - plugins.security.ssl.transport.enforce_hostname_verification=false
  Environment (par nœud) :
    - node.name=opensearch1 (2, 3)
  Ulimits :
    memlock : soft -1, hard -1
    nofile : soft 65536, hard 65536
  Volumes : opensearch{1,2,3}_data:/usr/share/opensearch/data
  Healthcheck :
    test : curl -fsSk https://localhost:9200/_cluster/health
    interval : 30s
  Deploy (par nœud) :
    replicas : 1
    placement : node.hostname == node{1,2,3} (anti-affinity)
    resources :
      limits : cpu 2.0, memory 2G
      reservations : cpu 1.0, memory 1G

OPENSEARCH-DASHBOARDS :
  Image : opensearchproject/opensearch-dashboards:2.15.0
  Environment :
    - OPENSEARCH_HOSTS=["https://opensearch1:9200","https://opensearch2:9200","https://opensearch3:9200"]
  Ports : 5601
  Labels Traefik :
    - traefik.enable=true
    - traefik.http.routers.opensearch-dashboards.rule=Host(`opensearch.${DOMAIN}`)
    - traefik.http.routers.opensearch-dashboards.entrypoints=websecure
    - traefik.http.routers.opensearch-dashboards.tls.certresolver=le
    - traefik.http.services.opensearch-dashboards.loadbalancer.server.port=5601

CASSANDRA (cluster 3 nœuds) :
  Image : cassandra:4.1.9
  Environment (commun) :
    - CASSANDRA_CLUSTER_NAME=${CASSANDRA_CLUSTER_NAME}
    - CASSANDRA_DC=${CASSANDRA_DC}
    - CASSANDRA_RACK=${CASSANDRA_RACK}
    - CASSANDRA_SEEDS=cassandra1,cassandra2,cassandra3
    - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
    - MAX_HEAP_SIZE=512M
    - HEAP_NEWSIZE=100M
  Volumes : cassandra{1,2,3}_data:/var/lib/cassandra
  Healthcheck :
    test : nodetool status | grep UN
    interval : 30s
    timeout : 10s
    retries : 5
  Deploy (par nœud) :
    replicas : 1
    placement : node.hostname == node{1,2,3}
    resources :
      limits : cpu 2.0, memory 2G
      reservations : cpu 1.0, memory 1G

CASSANDRA EXPORTER :
  Image : criteord/cassandra_exporter:2.3.8
  Environment :
    - CASSANDRA_HOST=cassandra1 (un par nœud ou discovery)
  Ports : 9404 (metrics)

PROMETHEUS :
  Image : prom/prometheus:v2.48.0
  Volumes :
    - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    - ./prometheus/rules:/etc/prometheus/rules:ro
    - prometheus_data:/prometheus
  Command :
    - --config.file=/etc/prometheus/prometheus.yml
    - --storage.tsdb.path=/prometheus
    - --storage.tsdb.retention.time=30d
    - --web.enable-lifecycle
  Ports : 9090
  Deploy :
    replicas : 1
    resources :
      limits : cpu 1.0, memory 2G

ALERTMANAGER :
  Image : prom/alertmanager:v0.25.0
  Volumes :
    - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
  Ports : 9093

GRAFANA :
  Image : grafana/grafana:9.5.2
  Environment :
    - GF_SECURITY_ADMIN_PASSWORD_FILE=/run/secrets/grafana_admin_password
    - GF_SERVER_ROOT_URL=https://grafana.${DOMAIN}
    - GF_INSTALL_PLUGINS=grafana-piechart-panel
  Volumes :
    - grafana_data:/var/lib/grafana
    - ./grafana/provisioning:/etc/grafana/provisioning:ro
  Secrets :
    - grafana_admin_password
  Ports : 3000
  Labels Traefik :
    - traefik.enable=true
    - traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN}`)
    - traefik.http.routers.grafana.entrypoints=websecure
    - traefik.http.routers.grafana.tls.certresolver=le

NODE-EXPORTER :
  Image : prom/node-exporter:v1.6.1
  Command :
    - --path.procfs=/host/proc
    - --path.sysfs=/host/sys
    - --path.rootfs=/rootfs
    - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
  Volumes :
    - /proc:/host/proc:ro
    - /sys:/host/sys:ro
    - /:/rootfs:ro
  Network_mode : host (ou pid:host + networks)
  Deploy :
    mode : global (un par nœud)

CADVISOR :
  Image : gcr.io/cadvisor/cadvisor:v0.47.2
  Volumes :
    - /:/rootfs:ro
    - /var/run:/var/run:rw
    - /sys:/sys:ro
    - /var/lib/docker/:/var/lib/docker:ro
  Ports : 8080
  Deploy :
    mode : global

BLACKBOX-EXPORTER :
  Image : prom/blackbox-exporter:v0.24.0
  Volumes :
    - ./blackbox/blackbox.yml:/etc/blackbox_exporter/config.yml:ro
  Ports : 9115

LOKI :
  Image : grafana/loki:2.8.1
  Volumes :
    - ./loki/loki-config.yml:/etc/loki/local-config.yaml:ro
    - loki_data:/loki
  Ports : 3100
  Environment :
    - LOKI_CONFIG_FILE=/etc/loki/local-config.yaml
  Command : -config.file=/etc/loki/local-config.yaml

PROMTAIL :
  Image : grafana/promtail:2.8.1
  Volumes :
    - ./promtail/promtail-config.yml:/etc/promtail/config.yml:ro
    - /var/lib/docker/containers:/var/lib/docker/containers:ro
    - /var/log:/var/log:ro
  Command : -config.file=/etc/promtail/config.yml
  Deploy :
    mode : global

MINIO :
  Image : minio/minio:RELEASE.2023-11-20T22-40-07Z
  Command : server /data --console-address ":9001" --compat
  Environment :
    - MINIO_ROOT_USER=${MINIO_ROOT_USER}
    - MINIO_ROOT_PASSWORD_FILE=/run/secrets/minio_root_password
  Volumes :
    - minio_data:/data
  Ports : 9000 (API), 9001 (Console)
  Healthcheck :
    test : curl -f http://localhost:9000/minio/health/live
    interval : 30s

RESTIC-BACKUP :
  Image : restic/restic:0.17.0
  Volumes :
    - db_data:/backup/db_data:ro
    - glpi_data:/backup/glpi_data:ro
    - grafana_data:/backup/grafana_data:ro
    - ./scripts:/scripts:ro
  Environment :
    - RESTIC_REPOSITORY=s3:http://minio:9000/${MINIO_BUCKET}
    - RESTIC_PASSWORD_FILE=/run/secrets/restic_password
    - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
    - AWS_SECRET_ACCESS_KEY_FILE=/run/secrets/minio_root_password
  Secrets :
    - restic_password
    - minio_root_password
  Entrypoint : /scripts/backup-cron.sh
  Deploy :
    replicas : 1
    placement : node.role == manager

CONSUL (dev only) :
  Image : consul:1.14
  Command : agent -dev -client=0.0.0.0
  Ports : 8500

FICHIERS DE CONFIGURATION À FOURNIR

1. prometheus.yml (scrape configs actuels + jobs manquants)
2. alertmanager.yml (routes, receivers email/pagerduty)
3. prometheus/rules/alert_rules.yml (seuils CPU, mémoire, disk, services down)
4. loki-config.yml (actuel + compactor, retention)
5. promtail-config.yml (actuel + labels service/stack/node)
6. traefik/dynamic/middlewares.yml (securityHeaders, ratelimit)
7. grafana/provisioning/datasources/*.yml (Prometheus, Loki)
8. grafana/provisioning/dashboards/*.json (4 dashboards minimum)
9. blackbox/blackbox.yml (modules http_2xx, tcp_connect)
10. cassandra/init.cql (keyspace exemple RF=3)
11. mariadb/init.sql (tables GLPI custom si nécessaire)
12. scripts/backup.sh (restic backup avec rétention GFS)
13. scripts/restore.sh (restic restore)
14. scripts/backup-cron.sh (planification 2h du matin)
15. nftables.conf (firewall hôte)
16. .env.example (toutes les variables)
17. Makefile (targets : up, down, deploy, backup, restore, logs, test)
18. README.md (procédures complètes)
19. PRA.md (plan de reprise détaillé)
20. docs/architecture.md (diagrammes Mermaid)
21. checks.sh (validation post-déploiement)

CONTRAINTES DE SÉCURITÉ

1. Réseaux segmentés (front/back/data) avec encryption overlay
2. Tous les secrets via Docker secrets
3. TLS obligatoire (HTTPS uniquement, certificats ACME)
4. Headers de sécurité Traefik (HSTS, CSP, X-Frame-Options)
5. Rate limiting Traefik (100 req/s)
6. Ressources CPU/mémoire limitées (deploy.resources)
7. Volumes en lecture seule quand possible (:ro)
8. Cap_drop : ALL pour CrowdSec
9. No new privileges
10. Healthchecks sur tous les services critiques
11. Pare-feu hôte nftables (ports 22, 80, 443, 2377, 7946, 4789 uniquement)
12. RBAC OpenSearch, Grafana
13. Passwords complexes (openssl rand -base64 32)
14. Rotation secrets documentée dans PRA

HAUTE DISPONIBILITÉ (PRODUCTION SWARM)

1. Docker Swarm : 3 managers (quorum), 2+ workers
2. Contraintes d'anti-affinité (spread par hostname/zone)
3. Réplicas : Traefik(2), GLPI(2), Grafana(2)
4. Rolling updates : parallelism=1, delay=30s, failure_action=rollback
5. Healthchecks avec retry et timeout adaptés
6. Stateful services : placement sur managers avec volumes persistants
7. Réseaux overlay avec encryption
8. Update_config et rollback_config pour tous les services

MONITORING : DASHBOARDS OBLIGATOIRES

1. Infra Overview :
   - Statut des services (up/down)
   - CPU/RAM/Disk par nœud
   - Trafic réseau
   - Erreurs 4xx/5xx Traefik
   - Expiration certificats (<14j)

2. Apps & Proxy :
   - Latences P50/P95/P99
   - Requests per second (RPS)
   - Backend saturation
   - Erreurs backend
   - CrowdSec bans

3. Cassandra :
   - État du cluster (UN/DN)
   - Pending compactions
   - Latences read/write P99
   - Dropped mutations
   - Heap usage

4. OpenSearch :
   - Cluster status (green/yellow/red)
   - Heap usage / GC
   - Shards (active/relocating/unassigned)
   - Indexing/search rate
   - Query latency

ALERTING : RÈGLES OBLIGATOIRES

1. Instance down (>1min)
2. CPU >85% (>5min)
3. Memory >85% (>5min)
4. Disk >85%
5. Certificat SSL <14 jours
6. OpenSearch cluster not green (>5min)
7. Cassandra node down
8. Quorum Swarm perdu
9. Backup failed
10. Database replication lag >60s

Receivers : email (Alertmanager), webhook (optionnel), PagerDuty (prod)

PLAN DE SAUVEGARDE (BACKUPS)

Stratégie GFS (Grandfather-Father-Son) :
- Quotidiennes : 7 jours
- Hebdomadaires : 4 semaines
- Mensuelles : 12 mois
- Annuelles : 3 ans

Services à sauvegarder :
1. MariaDB : mysqldump + binlogs
2. GLPI : files /var/www/html
3. Grafana : dashboards /var/lib/grafana
4. OpenSearch : Snapshot API vers MinIO
5. Cassandra : nodetool snapshot + commitlogs
6. Prometheus : optionnel (données volatiles)

Chiffrement : restic avec clé 256-bit
Destination : MinIO S3-compatible
Planification : cron 02h00 tous les jours
Rétention : restic forget --keep-* selon GFS
Tests de restauration : mensuels obligatoires

Scripts :
- backup.sh : déclenche toutes les sauvegardes
- restore.sh : restauration complète ou partielle
- backup-cron.sh : planification via crond

PRA : PLAN DE REPRISE D'ACTIVITÉ

Objectifs :
- RTO (Recovery Time Objective) : 2 heures
- RPO (Recovery Point Objective) : 1 heure (backups quotidiens)

Scénarios couverts :
1. Panne d'un nœud Swarm
2. Perte d'un volume Docker
3. Corruption base de données
4. Incident sécurité (compromission)
5. Perte de quorum Swarm
6. Expiration certificats
7. Coupure réseau/datacenter

Runbooks détaillés à fournir :
1. Restauration MariaDB depuis backup
2. Restauration OpenSearch depuis snapshot
3. Restauration Cassandra depuis snapshot
4. Reconstruction cluster Swarm
5. Re-génération certificats ACME
6. Rotation des secrets Docker
7. Rollback d'une mise à jour ratée

Checklist jour J :
- [ ] Déclaration incident
- [ ] Communication équipe
- [ ] Évaluation impact
- [ ] Décision restauration
- [ ] Exécution runbook
- [ ] Validation fonctionnelle
- [ ] Post-mortem

Tests DR :
- Trimestriels obligatoires
- Mesure RTO/RPO réels
- Documentation améliorations

VARIABLES D'ENVIRONNEMENT (.env.example)

# General
DOMAIN=example.com
EMAIL_ADMIN=admin@example.com
CLUSTER_NAME=bigdata
ENV=production

# Traefik / TLS
TRAEFIK_ACME_EMAIL=${EMAIL_ADMIN}
TRAEFIK_ENTRYPOINT_HTTP=web
TRAEFIK_ENTRYPOINT_HTTPS=websecure
CROWDSEC_BOUNCER_API_KEY=GENERATE_VIA_CSCLI

# GLPI / MariaDB
GLPI_DB_USER=glpi
GLPI_DB_NAME=glpi

# MinIO / Backup
MINIO_ROOT_USER=minioadmin
MINIO_BUCKET=backups
RESTIC_RETENTION_DAILY=7
RESTIC_RETENTION_WEEKLY=4
RESTIC_RETENTION_MONTHLY=12
RESTIC_RETENTION_YEARLY=3

# Prometheus/Alerting
ALERT_EMAILS=oncall@example.com,ops@example.com

# OpenSearch
OPENSEARCH_ADMIN_PASSWORD=ChangeMe_SecurePassword123!

# Cassandra
CASSANDRA_CLUSTER_NAME=bigdata_cassandra
CASSANDRA_DC=dc1
CASSANDRA_RACK=rack1

# Swarm (prod only)
SWARM_MANAGERS=3
SWARM_WORKERS=2

# Resource limits
MAX_CPU_LIMIT=2
MAX_MEM_LIMIT=2G

ARBORESCENCE DU DÉPÔT (COMPLÈTE)

bigdata-platform/
├── docker-compose.dev.yml
├── stack.prod.yml
├── .env
├── .env.example
├── .gitignore
├── Makefile
├── README.md
├── PRA.md
├── secrets/
│   ├── mariadb_root_password.txt
│   ├── glpi_db_password.txt
│   ├── grafana_admin_password.txt
│   ├── minio_root_password.txt
│   ├── restic_password.txt
│   └── opensearch_admin_password.txt
├── traefik/
│   ├── acme/
│   │   └── acme.json (600)
│   └── dynamic/
│       └── middlewares.yml
├── crowdsec/
│   ├── config/
│   │   └── (configs auto-générées)
│   └── data/
│       └── (db locale)
├── prometheus/
│   ├── prometheus.yml
│   └── rules/
│       └── alert_rules.yml
├── alertmanager/
│   └── alertmanager.yml
├── grafana/
│   └── provisioning/
│       ├── datasources/
│       │   ├── prometheus.yml
│       │   └── loki.yml
│       └── dashboards/
│           ├── dashboards.yml
│           ├── infra-overview.json
│           ├── apps-proxy.json
│           ├── cassandra.json
│           └── opensearch.json
├── loki/
│   └── loki-config.yml
├── promtail/
│   └── promtail-config.yml
├── blackbox/
│   └── blackbox.yml
├── mariadb/
│   └── init.sql
├── cassandra/
│   └── init.cql
├── opensearch/
│   ├── opensearch.yml (optionnel)
│   └── index-templates/
│       └── logs-template.json
├── scripts/
│   ├── backup.sh
│   ├── restore.sh
│   ├── backup-cron.sh
│   ├── init-swarm.sh
│   └── checks.sh
├── nftables/
│   └── nftables.conf
└── docs/
    ├── architecture.md
    ├── deployment.md
    └── runbooks/
        ├── restore-mariadb.md
        ├── restore-opensearch.md
        ├── restore-cassandra.md
        └── swarm-recovery.md

MAKEFILE : TARGETS OBLIGATOIRES

Targets à implémenter :
- make up : docker compose -f docker-compose.dev.yml up -d
- make down : docker compose -f docker-compose.dev.yml down
- make deploy : docker stack deploy -c stack.prod.yml bigdata
- make logs SERVICE=<name> : docker compose logs -f <name>
- make backup : lance ./scripts/backup.sh
- make restore : lance ./scripts/restore.sh
- make test : lance ./scripts/checks.sh
- make init-secrets : génère tous les secrets
- make init-swarm : initialise cluster Swarm
- make grafana-import : import dashboards
- make help : affiche l'aide

TESTS ET VALIDATION (checks.sh)

Vérifications automatiques :
1. Tous les conteneurs sont UP
2. Healthchecks passent (healthy)
3. Prometheus scrape tous les targets (up)
4. Alertmanager connecté à Prometheus
5. Grafana datasources connectés
6. Loki reçoit des logs
7. OpenSearch cluster green
8. Cassandra cluster UN (Up Normal) tous les nœuds
9. Traefik certificats valides (>14j)
10. Backups existent et <24h
11. CrowdSec LAPI répond
12. MariaDB replication OK (si répliqué)

Endpoints à tester :
- https://glpi.${DOMAIN}
- https://grafana.${DOMAIN}
- https://opensearch.${DOMAIN}
- http://localhost:9090/-/healthy (Prometheus)
- http://localhost:3100/ready (Loki)
- http://localhost:8080/dashboard/ (Traefik)

Statuts attendus : 200 OK, clusters GREEN/UN, healthy

HYPOTHÈSES ET VALEURS PAR DÉFAUT

1. Domaine : example.com (à remplacer dans .env)
2. Email admin : admin@example.com
3. Timezone : Europe/Paris
4. Retention Prometheus : 30 jours
5. Retention Loki : 30 jours (720h)
6. OpenSearch heap : 512m (ajuster selon RAM disponible)
7. Cassandra heap : 512m/100m (adjust selon RAM)
8. Backups : 02h00 tous les jours
9. Swarm : 3 managers, 2 workers minimum
10. TLS : Let's Encrypt prod (staging pour tests)
11. Rate limit Traefik : 100 req/s
12. Cassandra RF : 3, Consistency QUORUM
13. OpenSearch replicas : 1 (3 nœuds)

CRITÈRES D'ACCEPTATION FINAUX

[ OK ] Déploiement dev opérationnel : docker compose -f docker-compose.dev.yml up -d
[ OK ] Déploiement prod opérationnel : docker stack deploy -c stack.prod.yml bigdata
[ OK ] HTTPS via Traefik fonctionnel (certificats ACME valides)
[ OK ] CrowdSec + Bouncer actifs (vérifiable dans logs)
[ OK ] GLPI accessible et fonctionnel
[ OK ] OpenSearch cluster GREEN (3 nœuds)
[ OK ] Cassandra cluster UN (3 nœuds)
[ OK ] Prometheus scrape tous les exporters
[ OK ] Alertmanager reçoit et route les alertes
[ OK ] Grafana affiche les 4 dashboards
[ OK ] Loki reçoit les logs des conteneurs
[ OK ] Backup manuel réussi (make backup)
[ OK ] Restore testé et validé (make restore)
[ OK ] Checks.sh passe tous les tests
[ OK ] PRA complet, clair, avec runbooks détaillés
[ OK ] Documentation complète (README, architecture, runbooks)
[ OK ] Secrets générés et montés correctement
[ OK ] Pare-feu nftables configuré
[ OK ] Tous les fichiers fournis en blocs de code complets

NOTES FINALES

- Utiliser UNIQUEMENT des images officielles ou de confiance
- PINER toutes les versions (pas de :latest)
- Tous les fichiers doivent être fournis en blocs de code complets et prêts à l'emploi
- Commentaires clairs dans tous les fichiers de config
- Exemples réalistes (labels, variables, dashboards)
- Aucune étape manquante
- Code reproductible à l'identique
- Architecture production-ready avec HA et DR complets